#!/usr/bin/env python3
"""
AQIé æ¸¬ç³»çµ± - ä¸»åŸ·è¡Œè…³æœ¬ (é‡æ§‹ç‰ˆ)
Main Training Script with New Configuration System
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List, Optional
import json
from datetime import datetime

# æ·»åŠ srcç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent / "src"))

from src.config_manager import create_config_manager, get_available_pipelines, get_available_stations
from src.unified_config import UnifiedConfig
from src.unified_preprocessor import UnifiedPreprocessor  
from src.unified_window_generator import UnifiedWindowGenerator
from src.unified_trainer import UnifiedTrainer

def setup_logging() -> logging.Logger:
    """è¨­ç½®æ—¥å¿—"""
    logger = logging.getLogger("aqi_training_main")
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    
    return logger

def train_single_pipeline_station(pipeline_mode: str, station: Optional[str] = None) -> dict:
    """è¨“ç·´å–®å€‹ç®¡é“-æ¸¬ç«™çµ„åˆ"""
    logger = logging.getLogger(f"train_{pipeline_mode}_{station or 'global'}")
    
    try:
        # å‰µå»ºçµ±ä¸€é…ç½®
        config = UnifiedConfig(mode=pipeline_mode, station=station)
        logger.info(f"é–‹å§‹è¨“ç·´: {config}")
        
        # 1. æ•¸æ“šé è™•ç†
        logger.info("æ­¥é©Ÿ1: æ•¸æ“šé è™•ç†")
        preprocessor = UnifiedPreprocessor(config)
        preprocessor.process_full_pipeline()
        logger.info("âœ… é è™•ç†å®Œæˆ")
        
        # 2. æ™‚é–“çª—å£ç”Ÿæˆ
        logger.info("æ­¥é©Ÿ2: æ™‚é–“çª—å£ç”Ÿæˆ")
        window_generator = UnifiedWindowGenerator(config)
        window_generator.process_full_pipeline()
        logger.info("âœ… çª—å£ç”Ÿæˆå®Œæˆ")
        
        # 3. æ¨¡å‹è¨“ç·´
        logger.info("æ­¥é©Ÿ3: æ¨¡å‹è¨“ç·´")
        trainer = UnifiedTrainer(config)
        training_results = trainer.train_all_models()
        logger.info("âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        # 4. çµæœå½™æ•´
        result = {
            "pipeline_mode": pipeline_mode,
            "station": station,
            "timestamp": config.timestamp,
            "success": True,
            "models": training_results,
            "config_summary": {
                "time_settings": {
                    "window_size": config.time_config.window_size,
                    "horizon": config.time_config.horizon,
                    "train_ratio": config.time_config.train_ratio
                },
                "hardware": {
                    "use_gpu": config.performance_config.use_gpu,
                    "device": config.performance_config.device
                }
            }
        }
        
        logger.info(f"âœ… è¨“ç·´æˆåŠŸå®Œæˆ: {pipeline_mode} - {station or 'global'}")
        return result
        
    except Exception as e:
        error_msg = f"è¨“ç·´å¤±æ•—: {pipeline_mode} - {station or 'global'}: {e}"
        logger.error(error_msg)
        return {
            "pipeline_mode": pipeline_mode,
            "station": station,
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
        }

def train_pipeline_mode(pipeline_mode: str, stations: Optional[List[str]] = None) -> dict:
    """è¨“ç·´ç‰¹å®šç®¡é“æ¨¡å¼"""
    logger = logging.getLogger(f"train_pipeline_{pipeline_mode}")
    
    # ç¢ºå®šè¦è¨“ç·´çš„æ¸¬ç«™
    if pipeline_mode in ["combine", "combine_norm"]:
        # å…¨åŸŸæ¨¡å¼ï¼Œä¸éœ€è¦æŒ‡å®šæ¸¬ç«™
        stations_to_train = [None]
    else:
        # æ¸¬ç«™ç‰¹å®šæ¨¡å¼
        if stations is None:
            # ç²å–æ‰€æœ‰å¯ç”¨æ¸¬ç«™åˆ—è¡¨ï¼ˆå¾å¯¦éš›æ•¸æ“šæ–‡ä»¶ï¼‰
            if pipeline_mode == "separate":
                separate_dir = Path("data/raw/Separate")
                if separate_dir.exists():
                    station_files = list(separate_dir.glob("*_combined.csv"))
                    stations_to_train = [f.stem.split('_')[0] for f in station_files]
                    logger.info(f"å¾ Separate ç›®éŒ„ç™¼ç¾ {len(stations_to_train)} å€‹æ¸¬ç«™")
                else:
                    logger.error(f"Separate ç›®éŒ„ä¸å­˜åœ¨: {separate_dir}")
                    stations_to_train = []
            elif pipeline_mode == "separate_norm":
                separate_norm_dir = Path("data/raw/Separate_Nomorlization")
                if separate_norm_dir.exists():
                    station_files = list(separate_norm_dir.glob("*_combined.csv"))
                    # ä¿®å¾©ï¼šNomorlizationæª”åæ ¼å¼ç‚º Nomorlization_ç«™å_ä»£ç¢¼_combined.csv
                    stations_to_train = []
                    for f in station_files:
                        parts = f.stem.split('_')
                        if len(parts) >= 3 and parts[0] == "Nomorlization":
                            # æª”åæ ¼å¼ï¼šNomorlization_ç«™å_ä»£ç¢¼_combined
                            station_name = parts[1]
                            stations_to_train.append(station_name)
                        else:
                            # å›é€€ï¼šå‡è¨­ç‚ºæ¨™æº–æ ¼å¼ ç«™å_ä»£ç¢¼_combined
                            station_name = parts[0] if parts else f.stem
                            stations_to_train.append(station_name)
                    
                    # å»é‡ä¸¦æ’åº
                    stations_to_train = sorted(list(set(stations_to_train)))
                    logger.info(f"å¾ Separate_Nomorlization ç›®éŒ„ç™¼ç¾ {len(stations_to_train)} å€‹æ¸¬ç«™")
                else:
                    logger.error(f"Separate_Nomorlization ç›®éŒ„ä¸å­˜åœ¨: {separate_norm_dir}")
                    stations_to_train = []
            else:
                # å…¶ä»–æ¨¡å¼ï¼Œæš«æ™‚ä½¿ç”¨ç©ºåˆ—è¡¨ï¼ˆéœ€è¦å¾ŒçºŒå¯¦ç¾ï¼‰
                logger.warning(f"æ¨¡å¼ {pipeline_mode} çš„æ¸¬ç«™ç™¼ç¾å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
                stations_to_train = []
        else:
            stations_to_train = stations
    
    results = []
    for station in stations_to_train:
        logger.info(f"é–‹å§‹è¨“ç·´: {pipeline_mode} - {station or 'global'}")
        result = train_single_pipeline_station(pipeline_mode, station)
        results.append(result)
    
    # å½™ç¸½çµæœ
    successful_runs = [r for r in results if r.get("success", False)]
    failed_runs = [r for r in results if not r.get("success", False)]
    
    summary = {
        "pipeline_mode": pipeline_mode,
        "total_runs": len(results),
        "successful_runs": len(successful_runs),
        "failed_runs": len(failed_runs),
        "success_rate": len(successful_runs) / len(results) if results else 0,
        "results": results,
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
    }
    
    logger.info(f"ç®¡é“è¨“ç·´å®Œæˆ: {pipeline_mode}")
    logger.info(f"æˆåŠŸ: {len(successful_runs)}/{len(results)} ({summary['success_rate']:.1%})")
    
    return summary

def batch_training(pipeline_modes: List[str], stations: Optional[List[str]] = None) -> dict:
    """æ‰¹é‡è¨“ç·´å¤šå€‹ç®¡é“æ¨¡å¼"""
    logger = logging.getLogger("batch_training")
    logger.info("ğŸš€ é–‹å§‹æ‰¹é‡è¨“ç·´")
    logger.info(f"ç®¡é“æ¨¡å¼: {pipeline_modes}")
    logger.info(f"æ¸¬ç«™: {stations or 'é è¨­'}")
    
    all_results = {}
    
    for pipeline_mode in pipeline_modes:
        logger.info(f"=" * 60)
        logger.info(f"é–‹å§‹åŸ·è¡Œç®¡é“æ¨¡å¼: {pipeline_mode}")
        
        pipeline_result = train_pipeline_mode(pipeline_mode, stations)
        all_results[pipeline_mode] = pipeline_result
        
        if pipeline_result["success_rate"] > 0:
            logger.info(f"âœ… {pipeline_mode} å®Œæˆ ({pipeline_result['success_rate']:.1%} æˆåŠŸ)")
        else:
            logger.error(f"âŒ {pipeline_mode} å…¨éƒ¨å¤±æ•—")
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    total_runs = sum(r["total_runs"] for r in all_results.values())
    total_successful = sum(r["successful_runs"] for r in all_results.values())
    
    batch_summary = {
        "batch_training": True,
        "pipeline_modes": pipeline_modes,
        "stations": stations,
        "total_runs": total_runs,
        "successful_runs": total_successful,
        "overall_success_rate": total_successful / total_runs if total_runs > 0 else 0,
        "pipeline_results": all_results,
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
    }
    
    logger.info("=" * 60)
    logger.info("ğŸ‰ æ‰¹é‡è¨“ç·´å®Œæˆ!")
    logger.info(f"ç¸½é«”æˆåŠŸç‡: {batch_summary['overall_success_rate']:.1%} ({total_successful}/{total_runs})")
    
    return batch_summary

def save_results(results: dict, output_dir: str = "outputs/reports"):
    """ä¿å­˜è¨“ç·´çµæœ"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    timestamp = results.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
    
    if results.get("batch_training", False):
        filename = f"batch_training_results_{timestamp}.json"
    else:
        pipeline = results.get("pipeline_mode", "unknown")
        filename = f"{pipeline}_training_results_{timestamp}.json"
    
    output_file = output_path / filename
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“Š çµæœå·²ä¿å­˜: {output_file}")
    return output_file

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="AQIé æ¸¬ç³»çµ± - çµ±ä¸€è¨“ç·´åŸ·è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  # è¨“ç·´å–®å€‹ç®¡é“æ¨¡å¼
  python run_training.py --mode separate --stations æ¡ƒåœ’ å°åŒ—
  
  # è¨“ç·´å¤šå€‹ç®¡é“æ¨¡å¼
  python run_training.py --modes separate combine --stations æ¡ƒåœ’
  
  # æ‰¹é‡è¨“ç·´æ‰€æœ‰æ¨¡å¼
  python run_training.py --modes all
  
  # é©—è­‰é…ç½®
  python run_training.py --validate --mode separate --stations æ¡ƒåœ’
        """
    )
    
    # ç®¡é“é¸æ“‡åƒæ•¸
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        "--mode", 
        choices=get_available_pipelines() + ["all"],
        help="å–®å€‹ç®¡é“æ¨¡å¼"
    )
    mode_group.add_argument(
        "--modes", 
        nargs="+",
        choices=get_available_pipelines() + ["all"],
        help="å¤šå€‹ç®¡é“æ¨¡å¼"
    )
    
    # æ¸¬ç«™åƒæ•¸
    parser.add_argument(
        "--stations",
        nargs="+",
        help="æŒ‡å®šæ¸¬ç«™åˆ—è¡¨"
    )
    
    # å…¶ä»–åƒæ•¸
    parser.add_argument(
        "--validate",
        action="store_true",
        help="åƒ…é©—è­‰é…ç½®ï¼Œä¸åŸ·è¡Œè¨“ç·´"
    )
    
    parser.add_argument(
        "--output-dir",
        default="outputs/reports",
        help="çµæœè¼¸å‡ºç›®éŒ„"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è©³ç´°è¼¸å‡º"
    )
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥å¿—
    logger = setup_logging()
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # ç¢ºå®šè¦åŸ·è¡Œçš„ç®¡é“æ¨¡å¼
    if args.mode:
        pipeline_modes = [args.mode] if args.mode != "all" else get_available_pipelines()
    elif args.modes:
        if "all" in args.modes:
            pipeline_modes = get_available_pipelines()
        else:
            pipeline_modes = args.modes
    else:
        pipeline_modes = ["separate"]  # é è¨­æ¨¡å¼
    
    # é©—è­‰æ¸¬ç«™
    if args.stations:
        available_stations = get_available_stations()
        for station in args.stations:
            if station not in available_stations:
                logger.error(f"æ¸¬ç«™ä¸å­˜åœ¨: {station}")
                logger.info(f"å¯ç”¨æ¸¬ç«™: {available_stations}")
                sys.exit(1)
    
    # åƒ…é©—è­‰æ¨¡å¼
    if args.validate:
        logger.info("ğŸ” é…ç½®é©—è­‰æ¨¡å¼")
        for pipeline_mode in pipeline_modes:
            try:
                config = UnifiedConfig(mode=pipeline_mode, station=args.stations[0] if args.stations else None)
                logger.info(f"âœ… {pipeline_mode}: é…ç½®æœ‰æ•ˆ")
                logger.info(f"   æ™‚é–“è¨­ç½®: {config.time_config.window_size}h -> {config.time_config.horizon}h")
                logger.info(f"   ç¡¬é«”è¨­ç½®: GPU={config.performance_config.use_gpu}")
            except Exception as e:
                logger.error(f"âŒ {pipeline_mode}: é…ç½®ç„¡æ•ˆ - {e}")
        return
    
    # åŸ·è¡Œè¨“ç·´
    try:
        if len(pipeline_modes) == 1:
            # å–®å€‹ç®¡é“æ¨¡å¼
            results = train_pipeline_mode(pipeline_modes[0], args.stations)
        else:
            # æ‰¹é‡è¨“ç·´
            results = batch_training(pipeline_modes, args.stations)
        
        # ä¿å­˜çµæœ
        output_file = save_results(results, args.output_dir)
        
        # é¡¯ç¤ºæ‘˜è¦
        if results.get("batch_training", False):
            success_rate = results["overall_success_rate"]
        else:
            success_rate = results["success_rate"]
        
        if success_rate > 0.8:
            logger.info("ğŸ‰ è¨“ç·´åœ“æ»¿å®Œæˆ!")
        elif success_rate > 0.5:
            logger.warning("âš ï¸ è¨“ç·´éƒ¨åˆ†æˆåŠŸ")
        else:
            logger.error("âŒ è¨“ç·´å¤§éƒ¨åˆ†å¤±æ•—")
        
        sys.exit(0 if success_rate > 0 else 1)
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        logger.error(f"ğŸ’¥ è¨“ç·´åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 