#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨å·²æœ‰é è™•ç†æ–‡ä»¶é€²è¡Œæ™‚é–“çª—å£ç”Ÿæˆå’Œæ¨¡å‹è¨“ç·´
è·³éè·¯å¾‘å•é¡Œï¼Œç›´æ¥è™•ç†ç¾æœ‰æ•¸æ“š
"""

import os
import sys
import numpy as np
import torch
import lightgbm as lgb
import logging
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
import glob
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'direct_windows_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class DirectProcessor:
    """ç›´æ¥è™•ç†å™¨ï¼Œè™•ç†é è™•ç†æ•¸æ“šåˆ°æ¨¡å‹è¨“ç·´"""
    
    def __init__(self):
        self.processed_dir = Path("data/processed")
        self.windows_dir = Path("data/sliding_windows")
        self.models_dir = Path("models")
        self.reports_dir = Path("reports")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        for dir_path in [self.windows_dir, self.models_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def detect_processed_files(self) -> List[Path]:
        """æª¢æ¸¬æ‰€æœ‰å·²é è™•ç†çš„æ–‡ä»¶"""
        pattern = "separate_*_*_processed.npz"
        files = list(self.processed_dir.glob(pattern))
        logger.info(f"æª¢æ¸¬åˆ° {len(files)} å€‹é è™•ç†æ–‡ä»¶")
        return sorted(files, key=lambda f: f.stat().st_mtime, reverse=True)
    
    def extract_station_info(self, file_path: Path) -> Tuple[str, str]:
        """å¾æ–‡ä»¶åæå–æ¸¬ç«™å’Œæ™‚é–“æˆ³ä¿¡æ¯"""
        # æ–‡ä»¶åæ ¼å¼: separate_æ¸¬ç«™å_æ™‚é–“æˆ³_processed.npz
        parts = file_path.stem.split('_')
        if len(parts) >= 4:
            station = '_'.join(parts[1:-2])  # æ¸¬ç«™åå¯èƒ½åŒ…å«ä¸‹åŠƒç·š
            timestamp = parts[-2]  # æ™‚é–“æˆ³
            return station, timestamp
        else:
            raise ValueError(f"ç„¡æ³•è§£ææ–‡ä»¶å: {file_path}")
    
    def load_processed_data(self, file_path: Path) -> Dict:
        """è¼‰å…¥é è™•ç†æ•¸æ“š"""
        logger.info(f"è¼‰å…¥é è™•ç†æ•¸æ“š: {file_path}")
        
        data = np.load(file_path, allow_pickle=True)
        
        data_dict = {
            'X': data['X'],
            'y': data['y'],
            'dates': data['dates'],
            'metadata': data['metadata'].item()
        }
        
        logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆ: X={data_dict['X'].shape}, y={data_dict['y'].shape}")
        return data_dict
    
    def create_sliding_windows(self, X: np.ndarray, y: np.ndarray, 
                              window_size: int = 24, horizon: int = 6, stride: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """å‰µå»ºæ»‘å‹•æ™‚é–“çª—å£"""
        logger.info(f"å‰µå»ºæ»‘å‹•çª—å£: window_size={window_size}, horizon={horizon}, stride={stride}")
        
        n_samples, n_features = X.shape
        n_targets = y.shape[1]
        
        # è¨ˆç®—å¯ä»¥å‰µå»ºçš„çª—å£æ•¸é‡
        max_start = n_samples - window_size - horizon + 1
        n_windows = max(0, (max_start - 1) // stride + 1)
        
        if n_windows == 0:
            raise ValueError(f"æ•¸æ“šé•·åº¦ä¸è¶³ä»¥å‰µå»ºçª—å£: éœ€è¦è‡³å°‘ {window_size + horizon} å€‹æ¨£æœ¬ï¼Œå¯¦éš› {n_samples}")
        
        # åˆå§‹åŒ–çª—å£æ•¸çµ„
        X_windows = np.zeros((n_windows, window_size, n_features))
        y_windows = np.zeros((n_windows, horizon, n_targets))
        
        # å‰µå»ºçª—å£
        for i in range(n_windows):
            start_idx = i * stride
            end_idx = start_idx + window_size
            target_start = end_idx
            target_end = target_start + horizon
            
            X_windows[i] = X[start_idx:end_idx]
            y_windows[i] = y[target_start:target_end]
        
        logger.info(f"çª—å£å‰µå»ºå®Œæˆ: {n_windows} å€‹çª—å£")
        return X_windows, y_windows
    
    def split_data(self, X_windows: np.ndarray, y_windows: np.ndarray, 
                   train_ratio: float = 0.8, val_ratio: float = 0.1) -> Dict:
        """åˆ†å‰²æ•¸æ“šç‚ºè¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦é›†"""
        n_samples = X_windows.shape[0]
        
        # è¨ˆç®—åˆ†å‰²é»
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        splits = {
            'train': {
                'X': X_windows[:train_end],
                'y': y_windows[:train_end]
            },
            'val': {
                'X': X_windows[train_end:val_end],
                'y': y_windows[train_end:val_end]
            },
            'test': {
                'X': X_windows[val_end:],
                'y': y_windows[val_end:]
            }
        }
        
        logger.info(f"æ•¸æ“šåˆ†å‰²å®Œæˆ: train={splits['train']['X'].shape[0]}, "
                   f"val={splits['val']['X'].shape[0]}, test={splits['test']['X'].shape[0]}")
        
        return splits
    
    def save_windows_data(self, splits: Dict, station: str, timestamp: str) -> Path:
        """ä¿å­˜çª—å£æ•¸æ“š"""
        output_path = self.windows_dir / f"separate_{station}_{timestamp}_windows.npz"
        
        np.savez_compressed(
            output_path,
            X_train=splits['train']['X'],
            y_train=splits['train']['y'],
            X_val=splits['val']['X'],
            y_val=splits['val']['y'],
            X_test=splits['test']['X'],
            y_test=splits['test']['y']
        )
        
        logger.info(f"çª—å£æ•¸æ“šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def reshape_for_lgbm(self, X_windows: np.ndarray) -> np.ndarray:
        """ç‚ºLightGBMé‡æ–°å¡‘é€ æ•¸æ“š"""
        # LightGBMéœ€è¦2Dæ•¸æ“š (n_samples, n_features)
        # å°‡çª—å£æ•¸æ“šå¹³å±•: (n_windows, window_size, n_features) -> (n_windows, window_size * n_features)
        n_windows, window_size, n_features = X_windows.shape
        return X_windows.reshape(n_windows, window_size * n_features)
    
    def train_lgbm_model(self, splits: Dict, station: str, timestamp: str) -> Path:
        """è¨“ç·´LightGBMæ¨¡å‹"""
        logger.info(f"é–‹å§‹è¨“ç·´LightGBMæ¨¡å‹: {station}")
        
        # é‡æ–°å¡‘é€ æ•¸æ“š
        X_train = self.reshape_for_lgbm(splits['train']['X'])
        X_val = self.reshape_for_lgbm(splits['val']['X'])
        
        # å–ç¬¬ä¸€å€‹æ™‚é–“æ­¥çš„ç›®æ¨™å€¼ï¼ˆç°¡åŒ–ç‚ºå–®æ­¥é æ¸¬ï¼‰
        y_train = splits['train']['y'][:, 0, :]  # (n_samples, n_targets)
        y_val = splits['val']['y'][:, 0, :]
        
        # å¹³å±•ç›®æ¨™å€¼åˆ°1Dï¼ˆLightGBMä¸€æ¬¡åªèƒ½é æ¸¬ä¸€å€‹ç›®æ¨™ï¼‰
        # æˆ‘å€‘è¨“ç·´å¤šå€‹æ¨¡å‹ï¼Œæ¯å€‹ç›®æ¨™ä¸€å€‹
        models = {}
        n_targets = y_train.shape[1]
        
        for target_idx in range(n_targets):
            logger.info(f"è¨“ç·´ç›®æ¨™ {target_idx + 1}/{n_targets}")
            
            # LightGBMåƒæ•¸
            params = {
                'objective': 'regression',
                'metric': 'mae',
                'boosting_type': 'gbdt',
                'num_leaves': 20,  # è¼•é‡åŒ–
                'learning_rate': 0.1,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1,
                'max_bin': 255,
                'min_data_in_leaf': 50,
                'n_estimators': 100,  # è¼•é‡åŒ–
                'max_depth': 6,  # è¼•é‡åŒ–
                'random_state': 42
            }
            
            # è¨“ç·´æ¨¡å‹
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train, y_train[:, target_idx],
                eval_set=[(X_val, y_val[:, target_idx])],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            models[f'target_{target_idx}'] = model
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.models_dir / f"separate_{station}_{timestamp}_lgbm.pkl"
        joblib.dump(models, model_path)
        
        logger.info(f"LightGBMæ¨¡å‹å·²ä¿å­˜: {model_path}")
        return model_path
    
    def evaluate_model(self, model_path: Path, splits: Dict, station: str, timestamp: str) -> Dict:
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info(f"è©•ä¼°æ¨¡å‹æ€§èƒ½: {station}")
        
        # è¼‰å…¥æ¨¡å‹
        models = joblib.load(model_path)
        
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        X_test = self.reshape_for_lgbm(splits['test']['X'])
        y_test = splits['test']['y'][:, 0, :]  # å–ç¬¬ä¸€å€‹æ™‚é–“æ­¥
        
        # é€²è¡Œé æ¸¬
        predictions = {}
        metrics = {}
        
        for target_idx, (model_name, model) in enumerate(models.items()):
            y_pred = model.predict(X_test)
            y_true = y_test[:, target_idx]
            
            # è¨ˆç®—æŒ‡æ¨™
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
            
            predictions[model_name] = y_pred.tolist()
            metrics[model_name] = {
                'MAE': float(mae),
                'RMSE': float(rmse),
                'MAPE': float(mape)
            }
            
            logger.info(f"ç›®æ¨™ {target_idx}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%")
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_metrics = {
            'MAE': np.mean([m['MAE'] for m in metrics.values()]),
            'RMSE': np.mean([m['RMSE'] for m in metrics.values()]),
            'MAPE': np.mean([m['MAPE'] for m in metrics.values()])
        }
        
        results = {
            'station': station,
            'timestamp': timestamp,
            'model_path': str(model_path),
            'individual_metrics': metrics,
            'average_metrics': avg_metrics,
            'predictions': predictions
        }
        
        # ä¿å­˜è©•ä¼°çµæœ
        import json
        results_path = self.reports_dir / f"separate_{station}_{timestamp}_evaluation.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è©•ä¼°çµæœå·²ä¿å­˜: {results_path}")
        logger.info(f"å¹³å‡æ€§èƒ½: MAE={avg_metrics['MAE']:.4f}, RMSE={avg_metrics['RMSE']:.4f}, MAPE={avg_metrics['MAPE']:.2f}%")
        
        return results
    
    def process_single_station(self, file_path: Path) -> Dict:
        """è™•ç†å–®å€‹æ¸¬ç«™çš„å®Œæ•´pipeline"""
        try:
            # æå–æ¸¬ç«™ä¿¡æ¯
            station, timestamp = self.extract_station_info(file_path)
            logger.info(f"é–‹å§‹è™•ç†æ¸¬ç«™: {station} (æ™‚é–“æˆ³: {timestamp})")
            
            # è¼‰å…¥é è™•ç†æ•¸æ“š
            data_dict = self.load_processed_data(file_path)
            
            # å‰µå»ºæ»‘å‹•çª—å£
            X_windows, y_windows = self.create_sliding_windows(
                data_dict['X'], data_dict['y']
            )
            
            # åˆ†å‰²æ•¸æ“š
            splits = self.split_data(X_windows, y_windows)
            
            # ä¿å­˜çª—å£æ•¸æ“š
            windows_path = self.save_windows_data(splits, station, timestamp)
            
            # è¨“ç·´æ¨¡å‹
            model_path = self.train_lgbm_model(splits, station, timestamp)
            
            # è©•ä¼°æ¨¡å‹
            evaluation = self.evaluate_model(model_path, splits, station, timestamp)
            
            logger.info(f"âœ… æ¸¬ç«™ {station} è™•ç†å®Œæˆ")
            return {
                'status': 'success',
                'station': station,
                'timestamp': timestamp,
                'windows_path': windows_path,
                'model_path': model_path,
                'evaluation': evaluation
            }
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†æ¸¬ç«™å¤±æ•—: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'file_path': str(file_path)
            }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹ç›´æ¥è™•ç†æ‰€æœ‰æ¸¬ç«™çš„separate pipeline")
    
    processor = DirectProcessor()
    
    # æª¢æ¸¬æ‰€æœ‰é è™•ç†æ–‡ä»¶
    processed_files = processor.detect_processed_files()
    
    if not processed_files:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•é è™•ç†æ–‡ä»¶")
        return
    
    # éæ¿¾æ‰normæ–‡ä»¶ï¼Œåªè™•ç†åŸå§‹separateæ–‡ä»¶
    original_files = [f for f in processed_files if 'norm_' not in f.name]
    logger.info(f"æ‰¾åˆ° {len(original_files)} å€‹åŸå§‹separateé è™•ç†æ–‡ä»¶")
    
    # çµ±è¨ˆçµæœ
    results = []
    successful = 0
    failed = 0
    
    start_time = datetime.now()
    
    try:
        for i, file_path in enumerate(original_files, 1):
            logger.info(f"[{i}/{len(original_files)}] è™•ç†æ–‡ä»¶: {file_path.name}")
            
            result = processor.process_single_station(file_path)
            results.append(result)
            
            if result['status'] == 'success':
                successful += 1
            else:
                failed += 1
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ ç›´æ¥è™•ç†å®Œæˆ!")
        logger.info(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’")
        logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        logger.info(f"  - ç¸½æ–‡ä»¶æ•¸: {len(original_files)}")
        logger.info(f"  - æˆåŠŸè™•ç†: {successful}")
        logger.info(f"  - è™•ç†å¤±æ•—: {failed}")
        logger.info(f"  - æˆåŠŸç‡: {successful/len(original_files)*100:.1f}%")
        
        # ä¿å­˜è©³ç´°çµæœ
        import json
        summary = {
            'execution_info': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'execution_time': execution_time,
                'total_files': len(original_files),
                'successful': successful,
                'failed': failed
            },
            'detailed_results': results
        }
        
        output_file = f"direct_processing_results_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“ è©³ç´°çµæœå·²ä¿å­˜: {output_file}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 