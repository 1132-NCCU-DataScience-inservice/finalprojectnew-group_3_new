#!/usr/bin/env python3
"""
å®Œæˆæ‰€æœ‰æ¸¬ç«™separate pipelineçš„å‰©é¤˜æ­¥é©Ÿ
è™•ç†å·²å®Œæˆé è™•ç†çš„æ•¸æ“šï¼Œç¹¼çºŒé€²è¡Œæ™‚é–“çª—å£ç”Ÿæˆå’Œæ¨¡å‹è¨“ç·´
"""

import os
import sys
import numpy as np
import logging
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import glob

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.unified_window_generator import generate_windows_by_mode
from src.unified_trainer import train_models_by_mode

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'complete_separate_pipeline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def detect_preprocessed_stations() -> List[str]:
    """æª¢æ¸¬å·²é è™•ç†çš„æ¸¬ç«™"""
    processed_dir = Path("data/processed")
    stations = set()
    
    # å°‹æ‰¾ separate_*_*_processed.npz æ–‡ä»¶
    for file in processed_dir.glob("separate_*_*_processed.npz"):
        # æ–‡ä»¶åæ ¼å¼: separate_æ¸¬ç«™å_æ™‚é–“æˆ³_processed.npz
        parts = file.stem.split('_')
        if len(parts) >= 4:
            # æå–æ¸¬ç«™åï¼ˆå¯èƒ½åŒ…å«ä¸­æ–‡ï¼‰
            station_part = '_'.join(parts[1:-2])  # å»æ‰ 'separate' å’Œ æ™‚é–“æˆ³_processed
            stations.add(station_part)
    
    return sorted(list(stations))

def find_latest_preprocessed_file(station: str) -> Path:
    """æ‰¾åˆ°æ¸¬ç«™æœ€æ–°çš„é è™•ç†æ–‡ä»¶"""
    processed_dir = Path("data/processed")
    pattern = f"separate_{station}_*_processed.npz"
    files = list(processed_dir.glob(pattern))
    
    if not files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¸¬ç«™ {station} çš„é è™•ç†æ–‡ä»¶")
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
    files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    return files[0]

def process_station_windows(station: str) -> Dict:
    """è™•ç†å–®å€‹æ¸¬ç«™çš„æ™‚é–“çª—å£ç”Ÿæˆ"""
    try:
        logger.info(f"é–‹å§‹è™•ç†æ¸¬ç«™ {station} çš„æ™‚é–“çª—å£ç”Ÿæˆ")
        
        # ä½¿ç”¨çµ±ä¸€çš„æ™‚é–“çª—å£ç”Ÿæˆå™¨ï¼Œä½†æŒ‡å®šç‰¹å®šçš„é è™•ç†æ–‡ä»¶
        preprocessed_file = find_latest_preprocessed_file(station)
        logger.info(f"ä½¿ç”¨é è™•ç†æ–‡ä»¶: {preprocessed_file}")
        
        # ç›´æ¥èª¿ç”¨æ™‚é–“çª—å£ç”Ÿæˆ
        result = generate_windows_by_mode('separate', station)
        
        logger.info(f"âœ… æ¸¬ç«™ {station} æ™‚é–“çª—å£ç”ŸæˆæˆåŠŸ")
        return {'status': 'success', 'result': result}
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬ç«™ {station} æ™‚é–“çª—å£ç”Ÿæˆå¤±æ•—: {e}")
        return {'status': 'failed', 'error': str(e)}

def process_station_training(station: str) -> Dict:
    """è™•ç†å–®å€‹æ¸¬ç«™çš„æ¨¡å‹è¨“ç·´"""
    try:
        logger.info(f"é–‹å§‹è™•ç†æ¸¬ç«™ {station} çš„æ¨¡å‹è¨“ç·´")
        
        # ä½¿ç”¨çµ±ä¸€çš„æ¨¡å‹è¨“ç·´å™¨
        result = train_models_by_mode('separate', station)
        
        logger.info(f"âœ… æ¸¬ç«™ {station} æ¨¡å‹è¨“ç·´æˆåŠŸ")
        return {'status': 'success', 'result': result}
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬ç«™ {station} æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
        return {'status': 'failed', 'error': str(e)}

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ‰€æœ‰æ¸¬ç«™separate pipelineçš„å‰©é¤˜æ­¥é©Ÿ")
    
    # æª¢æ¸¬å·²é è™•ç†çš„æ¸¬ç«™
    stations = detect_preprocessed_stations()
    logger.info(f"æª¢æ¸¬åˆ° {len(stations)} å€‹å·²é è™•ç†çš„æ¸¬ç«™: {stations[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
    
    if not stations:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å·²é è™•ç†çš„æ¸¬ç«™æ•¸æ“š")
        return
    
    # çµ±è¨ˆçµæœ
    windows_results = {}
    training_results = {}
    
    start_time = datetime.now()
    
    try:
        # æ­¥é©Ÿ1: æ‰¹é‡æ™‚é–“çª—å£ç”Ÿæˆ
        logger.info(f"=" * 60)
        logger.info("æ­¥é©Ÿ1: æ‰¹é‡æ™‚é–“çª—å£ç”Ÿæˆ")
        logger.info(f"=" * 60)
        
        for i, station in enumerate(stations, 1):
            logger.info(f"[{i}/{len(stations)}] è™•ç†æ¸¬ç«™: {station}")
            windows_results[station] = process_station_windows(station)
        
        # çµ±è¨ˆæ™‚é–“çª—å£ç”Ÿæˆçµæœ
        windows_success = sum(1 for r in windows_results.values() if r['status'] == 'success')
        logger.info(f"âœ… æ™‚é–“çª—å£ç”Ÿæˆå®Œæˆ: {windows_success}/{len(stations)} æˆåŠŸ")
        
        # æ­¥é©Ÿ2: æ‰¹é‡æ¨¡å‹è¨“ç·´ï¼ˆåªè™•ç†æ™‚é–“çª—å£ç”ŸæˆæˆåŠŸçš„æ¸¬ç«™ï¼‰
        logger.info(f"=" * 60)
        logger.info("æ­¥é©Ÿ2: æ‰¹é‡æ¨¡å‹è¨“ç·´")
        logger.info(f"=" * 60)
        
        successful_stations = [s for s, r in windows_results.items() if r['status'] == 'success']
        logger.info(f"å°‡å° {len(successful_stations)} å€‹æ¸¬ç«™é€²è¡Œæ¨¡å‹è¨“ç·´")
        
        for i, station in enumerate(successful_stations, 1):
            logger.info(f"[{i}/{len(successful_stations)}] è¨“ç·´æ¸¬ç«™: {station}")
            training_results[station] = process_station_training(station)
        
        # çµ±è¨ˆæ¨¡å‹è¨“ç·´çµæœ
        training_success = sum(1 for r in training_results.values() if r['status'] == 'success')
        logger.info(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ: {training_success}/{len(successful_stations)} æˆåŠŸ")
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ æ‰€æœ‰æ¸¬ç«™separate pipelineåŸ·è¡Œå®Œæˆ!")
        logger.info(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’")
        logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        logger.info(f"  - ç¸½æ¸¬ç«™æ•¸: {len(stations)}")
        logger.info(f"  - æ™‚é–“çª—å£ç”ŸæˆæˆåŠŸ: {windows_success}")
        logger.info(f"  - æ¨¡å‹è¨“ç·´æˆåŠŸ: {training_success}")
        logger.info(f"  - å®Œæ•´pipelineæˆåŠŸ: {training_success}")
        
        # åˆ—å‡ºå¤±æ•—çš„æ¸¬ç«™
        failed_windows = [s for s, r in windows_results.items() if r['status'] == 'failed']
        failed_training = [s for s, r in training_results.items() if r['status'] == 'failed']
        
        if failed_windows:
            logger.warning(f"âš ï¸  æ™‚é–“çª—å£ç”Ÿæˆå¤±æ•—çš„æ¸¬ç«™ ({len(failed_windows)}): {failed_windows[:5]}...")
        
        if failed_training:
            logger.warning(f"âš ï¸  æ¨¡å‹è¨“ç·´å¤±æ•—çš„æ¸¬ç«™ ({len(failed_training)}): {failed_training[:5]}...")
        
        # ä¿å­˜è©³ç´°çµæœ
        import json
        results = {
            'execution_info': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'execution_time': execution_time,
                'total_stations': len(stations)
            },
            'windows_results': windows_results,
            'training_results': training_results,
            'summary': {
                'windows_success': windows_success,
                'training_success': training_success,
                'failed_windows': failed_windows,
                'failed_training': failed_training
            }
        }
        
        output_file = f"complete_separate_pipeline_results_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“ è©³ç´°çµæœå·²ä¿å­˜: {output_file}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 